{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from layers import *\n",
    "from networks import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "session = None\n",
    "def get_session():\n",
    "    global session\n",
    "    if session is None:\n",
    "        session = tf.InteractiveSession()\n",
    "    return session\n",
    "\n",
    "def initialize_params(params, sess):\n",
    "    initializable_variables = filter(lambda v: hasattr(v, 'initializer'), params.values())\n",
    "    if params is not None: sess.run(list(map(lambda v: v.initializer, initializable_variables)))\n",
    "        \n",
    "\n",
    "def eval_t(tensor, params=None, feed_dict={}):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        return sess.run(tensor, feed_dict)\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 3\n",
    "\n",
    "To practise working with Tensorflow and neural network architectures, we will now implement the popular ResNet bocks, with skip-connections.\n",
    "\n",
    "We will follow the proposed (to the right) from the first author of [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf), illustrated by this image:\n",
    "\n",
    "![Resnet block](https://camo.githubusercontent.com/7ae470c333cd760\n",
    "78e1c669055ad98bcedaf523f/68747470733a2f2f71696974612d696d6167652d7\n",
    "3746f72652e73332e616d617a6f6e6177732e636f6d2f302f3130303532332f6131\n",
    "3536613563322d303236622d646535352d613666622d6534666131373732623432632e706e67)\n",
    "\n",
    "You will do:\n",
    "- batch_norm\n",
    "- relu\n",
    "- conv2d\n",
    "- batch_norm\n",
    "- relu\n",
    "- conv2d\n",
    "\n",
    "Finally you add your filtered result to the incomming data.\n",
    "\n",
    "Implement this in the **resnet_block** in **layers.py**. **resnet_block** takes in a tuple of two filter sizes one for each convolution, and a tuple of two kernel size, one for each convolution. The **stride** parameter should only be used by the first **conv2d**, the other convolution should use **stride=1**.\n",
    "\n",
    "Since you are using batch_norm it is important to keep track of your update_ops (if your utilizing the is_training variable).\n",
    "\n",
    "## ResNet 3.1\n",
    "\n",
    "In this first part, ignore the params 'shortcut/W' and 'shortcut/b' and only use add operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-2388d91d5b91>:35: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Error should be less than 1e-6, and is: 2.921804e-09\n"
     ]
    }
   ],
   "source": [
    "#These first line is just to make sure you get the same results every run\n",
    "tf.reset_default_graph(); np.random.seed(1); tf.set_random_seed(1)\n",
    "\n",
    "#Initialzing variables\n",
    "loaded = np.load('test_weights/weights_31.npz')\n",
    "x = tf.convert_to_tensor(loaded['x'], dtype=tf.float32)\n",
    "\n",
    "#Use your resnet_block function\n",
    "resnet_out, params, update_op = resnet_block(x, filters=(2, 2))\n",
    "\n",
    "\n",
    "expected_output = [[[[ 0.93961203, -2.57712746],\n",
    "                   [-0.44419158, -2.3124783 ],\n",
    "                   [ 1.68305147,  0.27818435]],\n",
    "\n",
    "                  [[-0.3723861,  -4.24079657],\n",
    "                   [-4.86849117,  7.76358986],\n",
    "                   [-0.84846461, -1.19875765]],\n",
    "\n",
    "                  [[-0.65678322, -2.80952597],\n",
    "                   [ 0.98079479,  0.74412948],\n",
    "                   [-1.67484868, -1.9221983 ]]]]\n",
    "\n",
    "#Feeding in our own variables for your W and b\n",
    "feed_dict = {}\n",
    "for key in ('A/W', 'A/b', 'A/gamma', 'A/beta', 'B/W', 'B/b', 'B/gamma', 'B/beta'):\n",
    "    feed_dict[params[key]] = loaded[key.replace('/', '_')]\n",
    "\n",
    "\n",
    "output = eval_t(resnet_out, params, feed_dict)\n",
    "print('Error should be less than 1e-6, and is: %e' % \n",
    "      np.abs(output - expected_output).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 3.2 Changing number of channels\n",
    "\n",
    "If you just add the element-wise the input with the convolved result, you will run into problems if you have changed the number of channels in the filtered result.\n",
    "\n",
    "This is normally solved by convolving with a 1x1 filter, with the correct number of output channels. If you also change the size, you can use stride in the 1x1 convolution to compansate.\n",
    "\n",
    "Here you can test your algorithm with changing number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['A/W', 'shortcut/W', 'A/gamma', 'shortcut/b', 'B/W', 'B/beta', 'A/beta', 'B/b', 'B/gamma', 'A/b'])\n",
      "WARNING:tensorflow:From <ipython-input-1-2388d91d5b91>:35: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "[[[[ 1.6043973  -3.63098097]\n",
      "   [ 1.57434261  2.06497812]]\n",
      "\n",
      "  [[-2.62768459 -2.02865243]\n",
      "   [-0.96468055  1.30703235]]]]\n",
      "Error should be less than 1e-8, and is: 2.281952e-09\n"
     ]
    }
   ],
   "source": [
    "#These first line is just to make sure you get the same results every run\n",
    "tf.reset_default_graph(); np.random.seed(1); tf.set_random_seed(1)\n",
    "\n",
    "#Initialzing variables\n",
    "loaded = np.load('test_weights/weights_32.npz')\n",
    "x = tf.convert_to_tensor(loaded['x'], dtype=tf.float32)\n",
    "\n",
    "#Use your resnet_block function\n",
    "resnet_out, params, update_op = resnet_block(x, filters=(2, 2), stride=6)\n",
    "\n",
    "expected_output = [[[[ 1.6043973,  -3.63098097],\n",
    "                     [ 1.57434261,  2.06497812]],\n",
    "\n",
    "                    [[-2.62768459, -2.02865243],\n",
    "                     [-0.96468055,  1.30703235]]]]\n",
    "\n",
    "#Feeding in our own variables for your W and b\n",
    "feed_dict = {}\n",
    "print(params.keys())\n",
    "for key in ('A/W', 'A/b', 'A/gamma', 'A/beta', 'B/W', 'B/b', 'B/gamma', 'B/beta'):\n",
    "    feed_dict[params[key]] = loaded[key.replace('/', '_')]\n",
    "    \n",
    "output = eval_t(resnet_out, params, feed_dict)\n",
    "      \n",
    "assert output.shape == (1, 2, 2, 2), 'Output shape is wrong, are you sure you are \\\n",
    "filtering the shortcut path? Output shape%s' % str(output.shape)\n",
    "\n",
    "print('Error should be less than 1e-8, and is: %e' % \n",
    "      np.abs(output - expected_output).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 3.3 Deep residual network\n",
    "\n",
    "In this last exercise, we will train a network in the same way as we did with **deep_network_with_batchnorm**. Implement the function **deep_residual_network** in **networks**. This functions takes the same input as **deep_network_with_batchnorm** and returns the same output. Instead of using conv2d and batch_norm functions directly you can use the **resnet_block**, but remember that you still have to keep the update_ops.\n",
    "\n",
    "You can use the same number of filters for both convolutions in a resnet_block, to keep things simple.\n",
    "With limited number of compute power and network size you may not gain much of an improvement over using just batch normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cifar import load_cifar\n",
    "def scale_images(imgs):\n",
    "    imgs = imgs.astype(np.float32)\n",
    "    imgs -= imgs.min()\n",
    "    imgs /= imgs.max()\n",
    "    return imgs*2 - 1\n",
    "\n",
    "x_train, y_train = load_cifar()\n",
    "x_train = scale_images(x_train)\n",
    "x_test, y_test = load_cifar(test=True)\n",
    "x_test = scale_images(x_test)\n",
    "\n",
    "print('Dataset loaded with shapes:', x_train.shape, y_train.shape)\n",
    "print('Dataset loaded with shapes:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph();\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x_in = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y_in = tf.placeholder(tf.int32, [None])\n",
    "is_training = tf.Variable(True, dtype=tf.bool)\n",
    "\n",
    "\n",
    "number_of_classes = 10\n",
    "batch_size = 32\n",
    "\n",
    "filters=(16, 32, 64) #Change filters to improve your network\n",
    "strides=(2, 2, 2) #Change stride to improve your network\n",
    "\n",
    "y = tf.one_hot(y_in, number_of_classes)\n",
    "logits, loss, params, update_op = deep_network_with_batchnorm(x_in, y,\n",
    "                                                   number_of_classes=number_of_classes,\n",
    "                                                   filters=filters,\n",
    "                                                   strides=strides,\n",
    "                                                   is_training=is_training)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    ###########################\n",
    "    #### YOUR CODE HERE #######\n",
    "    # Write your code for training and testing the network\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    #running test and visualization\n",
    "    np.random.seed(1)\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for i in range(20):\n",
    "        batch_ind = np.random.choice(len(y_test), batch_size)\n",
    "        x_batch, y_batch = x_test[batch_ind], y_test[batch_ind]\n",
    "        loss_val, logit_vals = sess.run([loss, logits], {x_in:x_batch, y_in:y_batch, is_training:False})\n",
    "        test_loss.append(loss_val)\n",
    "        test_acc.append((logit_vals.argmax(1) == y_batch).mean())\n",
    "    print('TEST loss:', np.mean(test_loss), 'TEST accuracy:', np.mean(test_acc))\n",
    "    plt.plot(train_loss_history)\n",
    "    plt.plot(np.linspace(0, len(train_loss_history), len(test_loss_history)), test_loss_history)\n",
    "    assert np.mean(test_acc)>0.65"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
