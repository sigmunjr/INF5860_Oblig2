{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from layers import *\n",
    "from networks import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "session = None\n",
    "def get_session():\n",
    "    global session\n",
    "    if session is None:\n",
    "        session = tf.InteractiveSession()\n",
    "    return session\n",
    "\n",
    "def initialize_params(params, sess):\n",
    "    initializable_variables = filter(lambda v: hasattr(v, 'initializer'), params.values())\n",
    "    if params is not None: sess.run(list(map(lambda v: v.initializer, initializable_variables)))\n",
    "        \n",
    "\n",
    "def eval_t(tensor, params=None, feed_dict={}):\n",
    "    with tf.Session() as sess:\n",
    "        initialize_params(params, sess)\n",
    "        return sess.run(tensor, feed_dict)\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization 2.1\n",
    "In this exercise you will implement batch normalization in the function **batch_norm** in **layers.py**.\n",
    "\n",
    "In batch normalization you find the average and standard deviation over the batch dimension, and then normalize the input.\n",
    "\n",
    "![batch normalization](https://standardfrancis.files.wordpress.com/2015/04/screenshot-from-2015-04-16-133436.png?w=1008)\n",
    "\n",
    "\n",
    "\n",
    "You should have one average and one mean value for each input channel and each spatial position. You should also create one *gamma* and one *beta* variable with the same shape. After the normalization (subtract *mean* and divide by *standard deviation*), you should multiply with *gamma* and finally add *beta*.\n",
    "\n",
    "*gamma* should be initialized to 1 and *beta* to zero, but they should both be trainable.\n",
    "\n",
    "In this first part of the exercise you can just ignore the variable **is_training**, and just implement the case, where you are training your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); np.random.seed(1); tf.set_random_seed(1)\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.uniform(0, 1000, size=(2, 12, 12, 3)), dtype=tf.float32)\n",
    "W = np.random.normal(size=(3, 3, 3, 2))\n",
    "b = np.random.normal(size=(2,))\n",
    "\n",
    "batch_out, params, update_ops = batch_norm(x)\n",
    "\n",
    "output = eval_t(batch_out, params)\n",
    "assert np.abs(output.mean()) < 1e-6 and np.abs(1 - output.std()) < 1e-6, \\\n",
    "    'Batchnorm is not normalizing properly. Expected zero mean and unit standard deviation, got: (%f, %f)'\\\n",
    "    % (output.mean(), output.std())\n",
    "print(output.mean(), output.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization under testing 2.2\n",
    "Normally you don't want your result to be dependent on the other images in the batch. We also like to get the same result everytime we send in the same image.\n",
    "\n",
    "Under testing with batch normalization, we usually use an accumulated **mean** and **standard deviation** for the whole training set, instead of just the incomming batch. To implement this we keep an *exponentially smoothed* average of both variables.\n",
    "\n",
    "For this exercise we will give you most of the part, since it is quite *tensorflow* technical. If you find this hard, we recommend you to skip this part and try again after you are finished with the rest.\n",
    "\n",
    "We initialize *average_mean* and *average_var* to zero:\n",
    "\n",
    "``` python\n",
    "  average_mean = tf.Variable(tf.zeros_like(batch_mean))\n",
    "  average_var = tf.Variable(tf.zeros_like(batch_var))\n",
    "```\n",
    "\n",
    "then we create assign operators, so we can update them during training:\n",
    "\n",
    "``` python\n",
    "  assign_mean = average_mean.assign(average_mean*0.85 + batch_mean*0.15)\n",
    "  assign_var = average_var.assign(average_var*0.85 + batch_var*0.15)\n",
    "```\n",
    "\n",
    "**OBS**! Remember that assign operators is not run at once, but run every time you do sess.run(assign_mean) or sess.run(assign_var)\n",
    "\n",
    "Finally to chooses you can use **tf.cond** to choose what **mean** and **variance** you will be using based on the variable **is_training_tensor**.\n",
    "\n",
    "``` python\n",
    "  mean, var = tf.cond(is_training_tensor, lambda: (batch_mean, batch_var), lambda: (average_mean, average_var))\n",
    "```\n",
    "\n",
    "Here **tf.cond** return the batch_mean and batch_var if is_training_tensor=True and the averages otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); np.random.seed(1); tf.set_random_seed(1)\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.uniform(0, 1000, size=(2, 12, 12, 3)), dtype=tf.float32)\n",
    "W = np.random.normal(size=(3, 3, 3, 2))\n",
    "b = np.random.normal(size=(2,))\n",
    "\n",
    "batch_out, params, update_op = batch_norm(x, is_training=False)\n",
    "with tf.Session() as sess:\n",
    "    initialize_params(params, sess)\n",
    "    b1 = sess.run(batch_out)\n",
    "    print('Standard deviation of output before update', b1.std() )\n",
    "\n",
    "    for i in range(50):\n",
    "        sess.run(update_op)\n",
    "\n",
    "    b2 = sess.run(batch_out)\n",
    "    print('Standard deviation of output after update', b2.std() )\n",
    "    output = sess.run(batch_out)# eval_t(batch_out, params)\n",
    "    assert np.abs(output.mean()) < 5.0  and np.abs(1 - output.std()) < 5.0, \\\n",
    "        'Batchnorm is not normalizing properly. Expected zero mean and unit standard deviation, got: (%f, %f)'\\\n",
    "        % (output.mean(), output.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep network with batch normalization 2.3\n",
    "Build your network in the same way as in 1.3, but this time include your **batch_norm** function before every convolutional layer.\n",
    "\n",
    "Update our **parameters** from your **conv2d** layer with the **batch_norm** parameters, so you also get **'conv1/gamma'**, **'conv1/beta'** etc.\n",
    "\n",
    "With batch normalization it is important that you also run your **update_op**. At the end of **deep_network_with_batchnorm**, the update_ops are grouped, but you have to put all **update_ops** from the **batch_norm** layer into the list called **update_ops**.\n",
    "\n",
    "Then when you train your network, you have to remembre to also run your **update_op**.\n",
    "\n",
    "To get everything right with *batch normalization* may be tricky. I therefore recommend that you try without the \"testing mode\" first. You can simply feed in *is_training: True* everywhere, and you don't have to mind about updates and all the other complications.\n",
    "\n",
    "If you do infact use *is_training: True*, when testing, i recommend using as large batch-size as possible.\n",
    "\n",
    "If you get everything right with the batch norm, you should experience that your model get *better results*, *more robust to parameter changes* and *have a smaller difference between training and test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cifar import load_cifar\n",
    "def scale_images(imgs):\n",
    "    imgs = imgs.astype(np.float32)\n",
    "    imgs -= imgs.min()\n",
    "    imgs /= imgs.max()\n",
    "    return imgs*2 - 1\n",
    "\n",
    "x_train, y_train = load_cifar()\n",
    "x_train = scale_images(x_train)\n",
    "x_test, y_test = load_cifar(test=True)\n",
    "x_test = scale_images(x_test)\n",
    "\n",
    "print('Dataset loaded with shapes:', x_train.shape, y_train.shape)\n",
    "print('Dataset loaded with shapes:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph();\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x_in = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y_in = tf.placeholder(tf.int32, [None])\n",
    "is_training = tf.Variable(True, dtype=tf.bool)\n",
    "\n",
    "\n",
    "number_of_classes = 10\n",
    "batch_size = 32\n",
    "\n",
    "filters = (16, 32, 64) #Change filters to improve your network\n",
    "strides = (2, 2, 2) #Change stride to improve your network\n",
    "\n",
    "y = tf.one_hot(y_in, number_of_classes)\n",
    "logits, loss, params, update_op = deep_network_with_batchnorm(x_in, y,\n",
    "                                                   number_of_classes=number_of_classes,\n",
    "                                                   filters=filters,\n",
    "                                                   strides=strides,\n",
    "                                                   is_training=is_training)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    \n",
    "    #Write your code for training and testing\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for i in range(20):\n",
    "        batch_ind = np.random.choice(len(y_test), batch_size)\n",
    "        x_batch, y_batch = x_test[batch_ind], y_test[batch_ind]\n",
    "        loss_val, logit_vals = sess.run([loss, logits], {x_in:x_batch, y_in:y_batch, is_training:False})\n",
    "        test_loss.append(loss_val)\n",
    "        test_acc.append((logit_vals.argmax(1) == y_batch).mean())\n",
    "    print('TEST loss:', np.mean(test_loss), 'TEST accuracy:', np.mean(test_acc))\n",
    "    plt.plot(train_loss_history)\n",
    "    plt.plot(np.linspace(0, len(train_loss_history), len(test_loss_history)), test_loss_history)\n",
    "    assert np.mean(test_acc)>0.67"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
