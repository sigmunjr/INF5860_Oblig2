{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from layers import *\n",
    "from networks import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "session = None\n",
    "def get_session():\n",
    "    global session\n",
    "    if session is None:\n",
    "        session = tf.InteractiveSession()\n",
    "    return session\n",
    "\n",
    "def initialize_params(params, sess):\n",
    "    initializable_variables = filter(lambda v: hasattr(v, 'initializer'), params.values())\n",
    "    if params is not None: sess.run(list(map(lambda v: v.initializer, initializable_variables)))\n",
    "        \n",
    "\n",
    "def eval_t(tensor, params=None, feed_dict={}):\n",
    "    with tf.Session() as sess:\n",
    "        initialize_params(params, sess)\n",
    "        out = sess.run(tensor, feed_dict)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepNetwork 1\n",
    "\n",
    "\n",
    "In this exercise you will implement a simple convolutional network in Tensorflow. In **layers.py** you should implement the functions **conv2d** and **fully_connected_layer**.\n",
    "\n",
    "## Conv2D 1.1\n",
    "First implement **conv2d**, by using the **tf.nn.conv2d** function directly. Your **conv2d** function should both do a convolution with a trainable kernel and add a trainable bias.\n",
    "\n",
    "Your kernel should be stored in params['W'] and your bias in params['b']\n",
    "\n",
    "You should use padding, so your output size is only affected by the **stride** and not your **kernel size**. You can use the built in parameter **padding=** for automatic padding.\n",
    "\n",
    "Try to use some reasonable initialization scheme, e.g. xavier initialization.\n",
    "\n",
    "First you can run the following *cell* just to make sure you get your shapes right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These first line is just to make sure you get the same results every run\n",
    "tf.reset_default_graph(); np.random.seed(1); tf.set_random_seed(1)\n",
    "\n",
    "B, W, H, C = 5, 30, 30, 8\n",
    "conv_out, params = conv2d(tf.random_normal((B, W, H, C)), 3, stride=2)\n",
    "print('Expected output shape:', [5, 15, 15, 3] , \\\n",
    "    ', output shape', conv_out.get_shape().as_list())\n",
    "\n",
    "\n",
    "B, W, H, C = 2, 41, 41, 5\n",
    "conv_out, params = conv2d(tf.random_normal((B, W, H, C)), 6, stride=1)\n",
    "print('Expected output shape:', [2, 41, 41, 6] , \\\n",
    "    ', output shape', conv_out.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if your graph is right\n",
    "In this cell we run your graph, but with our own *weights*. With this *cell* you can make sure\n",
    "you made your graph right, independent of your initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These first line is just to make sure you get the same results every run\n",
    "tf.reset_default_graph(); np.random.seed(1); tf.set_random_seed(1)\n",
    "\n",
    "#Initialzing variables\n",
    "loaded_weights = np.load('test_weights/weights_11.npz')\n",
    "x, W, b = loaded_weights['x'], loaded_weights['W'], loaded_weights['b']\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "\n",
    "\n",
    "#Use your function for convolution\n",
    "conv_out, params = conv2d(x, 2, stride=6)\n",
    "\n",
    "expected_output = [[[[-1.72651875,  2.8096242 ],\n",
    "                   [-1.68763387,  0.69264042]],\n",
    "\n",
    "                  [[-1.79062819,  3.47469163],\n",
    "                   [-2.8855257,   3.12237906]]],\n",
    "\n",
    "\n",
    "                 [[[-1.38491416,  1.87461901],\n",
    "                   [-2.847018,    4.49191284]],\n",
    "\n",
    "                  [[-2.84181547,  1.58565772],\n",
    "                   [-2.51808834,  3.25931454]]]]\n",
    "\n",
    "#Feeding in our own variables for your W and b\n",
    "print(conv_out)\n",
    "feed_dict = {params['W']: W, params['b']: b}\n",
    "output = eval_t(conv_out, params, feed_dict)\n",
    "print(output.mean(), output.std(), np.sqrt(2))\n",
    "print('Error should be less than 1e-8, and is: %e' % \n",
    "      np.abs(output - expected_output).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected layer 1.2\n",
    "\n",
    "Now you should implement the **fully_connected_layer** function in **layers.py**, by using the **tf.matmul** function directly.\n",
    "\n",
    "Try to use some reasonable initialization scheme, e.g. xavier initialization.\n",
    "\n",
    "The input to fully_connected_layer can be both 2D and 4D. Try to keep you implementation independant of batch-size; you can use different batch size in different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); np.random.seed(3); tf.set_random_seed(1)\n",
    "\n",
    "loaded_weights = np.load('test_weights/weights_12.npz')\n",
    "x, W, b = loaded_weights['x'], loaded_weights['W'], loaded_weights['b']\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "\n",
    "conv_out, params = fully_connected_layer(x, 2)\n",
    "\n",
    "expected_output = [[ 18.78108597,   0.54020518],\n",
    "                   [  8.42805862,  -3.65233493]]\n",
    "\n",
    "feed_dict = {params['W']: W, params['b']: b}\n",
    "output = eval_t(conv_out, params, feed_dict)\n",
    "print('Error should be less than 1e-8, and is: %e' % \n",
    "      np.abs(output - expected_output).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepNetwork 1.3\n",
    "### Building the network\n",
    "Now it's time to put these parts together. Implement the **deep_network** function in **networks.py**.\n",
    "The **deep_network** function takes a list of *filter numbers*. If **N** is the length of the *filter number* list, then you should have **N** convolutional layers. Each *convolutional layer* output the number of channels corresonding to the place in the list.\n",
    "\n",
    "If *filters=(16, 32)*, then you should build **2** convolutional layers, where the output of the first layer have **16** channels and the second layer have **32** channels.\n",
    "\n",
    "The function also takes a list of strides as input of the same length as *filters*. These values correspond to the stride for each convolutional layer.\n",
    "\n",
    "If *strides=(1, 2)*, then the first layer should have stride *1* and the second layer stride *2*. You should only apply stride in the spatial dimensions and always the same stride for height and width.\n",
    "\n",
    "After each convolutional layer, add a **relu** layer. You can use **tf.nn.relu**.\n",
    "\n",
    "Finally you should add a **fully_connected_layer** at the end, with outputsize (batch_size, number_of_classes). The fully_connected_layer are not followed by a relu layer.\n",
    "\n",
    "To simplify things we will just use **3x3** convolutions.\n",
    "\n",
    "### Saving the parameters\n",
    "Save all the parameters in the network in the *params* dictionary. The convolutional layers should be saved with the keys **'conv`<layer_number`>/W'** and **'conv`<layer_number`>/b'**, where you index *layer_number* from 1.\n",
    "\n",
    "So the first layer will have keys **'conv1/W'** and **'conv1/b'** and so on.\n",
    "\n",
    "The fully connected layer should use the keys **'fc/W'** and **'fc/b'**.\n",
    "\n",
    "Run the following *cell* to check that the network have reasonable output values and correct shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); np.random.seed(1);\n",
    "x = tf.convert_to_tensor(np.random.normal(size=(2, 12, 12, 3)), dtype=tf.float32)\n",
    "W = np.random.normal(size=(3, 3, 3, 16))\n",
    "b = np.random.normal(size=(16,))\n",
    "\n",
    "conv_out, params = deep_network(x)\n",
    "\n",
    "output = eval_t(conv_out, params)\n",
    "\n",
    "print('Output mean:', output.mean(), 'output std:', output.std())\n",
    "\n",
    "assert output.shape == (2, 2), 'Output shape is %s, but should be (2, 2)' % str(output.shape)\n",
    "\n",
    "assert 1e-2 < np.abs(output).mean() < 2, \\\n",
    "'Output is out of expected range, is initialization correct? Average output range = %f' % np.abs(output).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inital check of the network\n",
    "\n",
    "In the following cell we check that the network give similar results as expected, if we feed in our own weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); np.random.seed(1); tf.set_random_seed(1)\n",
    "loaded = np.load('test_weights/weights_13.npz')\n",
    "\n",
    "W, b = loaded['W'], loaded['b']\n",
    "x = tf.convert_to_tensor(loaded['x'], dtype=tf.float32)\n",
    "\n",
    "filters = (8, 16, 16, 32, 32, 64, 64)\n",
    "strides = (2,  1,  2,  1,  2,  1, 2)\n",
    "\n",
    "conv_out, params = deep_network(x, number_of_classes=3, filters=filters, strides=strides)\n",
    "\n",
    "feed_dict = {}\n",
    "filters_prev = x.get_shape().as_list()[-1]\n",
    "\n",
    "for i, f in enumerate(filters):\n",
    "    keyW = params['conv%d/W' % (i+1)]\n",
    "    keyb = params['conv%d/b' % (i+1)]\n",
    "    feed_dict[keyW] = loaded['conv%d_W' % (i+1)]\n",
    "    feed_dict[keyb] = loaded['conv%d_b' % (i+1)]\n",
    "    \n",
    "    assert tuple(keyW.get_shape().as_list()) == feed_dict[keyW].shape, \\\n",
    "        'Expected shape of %s to be %s, but it got %s' % (keyW.name,\n",
    "                                                          str(tuple(keyW.get_shape().as_list())),\n",
    "                                                          str(feed_dict[keyW].shape))\n",
    "    \n",
    "    filters_prev = f\n",
    "\n",
    "#Setting variables for fully connected\n",
    "feed_dict[params['fc/W']] = W\n",
    "feed_dict[params['fc/b']] = b\n",
    "\n",
    "expected_output = [[-1195626.375,    -1342853.25,      -304015.96875 ],\n",
    "                   [-1032230.8125,    -1197741.75,       -66713.984375]]\n",
    "\n",
    "output = eval_t(conv_out, params, feed_dict)\n",
    "\n",
    "assert output.shape == (2, 3), 'Output shape is %s, but should be (2, 3)' % str(output.shape)\n",
    "\n",
    "difference = np.abs(output - expected_output).mean()\n",
    "print('The output differed from exected with %e, should be less than 1' % (difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check loss\n",
    "Here you can check if the untrained network give a logical inital loss. This will depend on your initialization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); np.random.seed(1); tf.set_random_seed(1)\n",
    "x = tf.convert_to_tensor(np.random.normal(size=(100, 12, 12, 3)), dtype=tf.float32)\n",
    "y = tf.convert_to_tensor(np.random.randint(0, 10, size=(100,)), dtype=tf.int32)\n",
    "y = tf.one_hot(y, 10)\n",
    "\n",
    "W = np.random.normal(size=(3, 3, 3, 16))\n",
    "b = np.random.normal(size=(16,))\n",
    "\n",
    "logits, loss, params = deep_network(x, y, number_of_classes=10)\n",
    "loss_val = eval_t(loss, params)\n",
    "\n",
    "print('Initial loss:', loss_val)\n",
    "\n",
    "assert loss_val < 3.0,\\\n",
    "    'A resonable initial loss should be less than 3, for 10 classes, you got %f' % loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cifar-10 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cifar import load_cifar\n",
    "def scale_images(imgs):\n",
    "    imgs = imgs.astype(np.float32)\n",
    "    imgs -= imgs.min()\n",
    "    imgs /= imgs.max()\n",
    "    return imgs*2 - 1\n",
    "\n",
    "x_train, y_train = load_cifar()\n",
    "x_train = scale_images(x_train)\n",
    "x_test, y_test = load_cifar(test=True)\n",
    "x_test = scale_images(x_test)\n",
    "\n",
    "print('Dataset loaded with shapes:', x_train.shape, y_train.shape)\n",
    "print('Dataset loaded with shapes:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a deep network 1.4\n",
    "Crate a optimizer and initialize you variables. Then train your network. Try to get at least **65%** accuracy.\n",
    "\n",
    "Try to run both training and test, and fill the *train_loss_history* and *test_loss_history* with the corresponding losses for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph();\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x_in = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y_in = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "number_of_classes = 10\n",
    "batch_size = 32\n",
    "\n",
    "filters=(16, 32, 64) #Change filters to improve your network\n",
    "strides=(2, 1, 2) #Change stride to improve your network\n",
    "\n",
    "y = tf.one_hot(y_in, number_of_classes)\n",
    "logits, loss, params = deep_network(x_in, y, number_of_classes=number_of_classes)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    #Write your code for training and testing\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "    np.random.seed(1)\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for i in range(20):\n",
    "        batch_ind = np.random.choice(len(y_test), batch_size)\n",
    "        x_batch, y_batch = x_test[batch_ind], y_test[batch_ind]\n",
    "        loss_val, logit_vals = sess.run([loss, logits], {x_in:x_batch, y_in:y_batch})\n",
    "        test_loss.append(loss_val)\n",
    "        test_acc.append((logit_vals.argmax(1) == y_batch).mean())\n",
    "    print('TEST loss:', np.mean(test_loss), 'TEST accuracy:', np.mean(test_acc))\n",
    "    plt.plot(train_loss_history)\n",
    "    plt.plot(np.linspace(0, len(train_loss_history), len(test_loss_history)), test_loss_history)\n",
    "    assert np.mean(test_acc)>0.65"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
